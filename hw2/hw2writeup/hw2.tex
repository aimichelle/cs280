\documentclass[11pt]{article}
\usepackage{amsmath,textcomp,amssymb,graphicx,alltt}
\usepackage[top=1in, bottom=1in, left=.5in, right=.5in]{geometry}

\def\Name{Allen Tang, Michelle Nguyen}  % Your name

\title{CS280 -- Spring 2015 -- Homework 2}
\author{\Name}
\markboth{CS280 -- Spring 2015 Homework 2 \Name}{CS174 -- Spring 2015 Homework 2 \Name}

\pagestyle{myheadings}
\begin{document}
\maketitle

\section*{1.1 Q1}
THE GRAPHIC IS 'Question1.1.png'
\newpage
\section*{1.2 Q2}
\begin{itemize}
\item[a)]
Spatial pyramids give a regional "density" to the image. Some digits such as $4$ and $9$ may be classified similarly if we just use raw pixels, but if we use spatial pyramids, we are able to detect that many $4$'s have an opening on the top of the digit, where $9$'s do not. This difference can be captured by a spatial pyramid.
\item[b)]
THE GRAPHIC IS 'Question1.2b1.png'
THE GRAPHIC IS 'Question1.2b2.png'
We get roughly a $0.035\,(22\%)$ decrease on the 10,000 training sample set in error rate, which is significant.
\end{itemize}
\newpage


\newpage
\section*{1.3 Q3}
\pmb{Note: When comparing error rates, we chose the c (same across both trials) that obtains the lowest error rate.}	
\begin{itemize}
\item[a)]
Gradient orientations should help because they capture the relative ratios of gradient directions, which is invariant to slant (kind of like a shear).
\item[b)]
THE GRAPHIC IS 'Question1.3b1.png'
THE GRAPHIC IS 'Question1.3b2.png'
We obtain a decrease in error rate of about $0.1\,(81\%)$ on the 10,000 training sample set, which is a tremendous increase in performance.
\item[c)]
THE GRAPHIC IS 'Question1.3c1.png'
THE GRAPHIC IS 'Question1.3c2.png'
The error rate drops by 0.017 on the 10,000 training sample set. Normalizing the histograms  likely helps when considering two digits that are similar except for scaling. If we do not normalize the histograms, the gradient angles of the two digits would fall in the same bins but differ in magnitude. Normalizing the two histograms before making them features would make the features closer to each other. Since they have the same label, the probability that both are classified correctly may increase.
\item[d)]
THE GRAPHIC IS 'Question1.3d1.png'
THE GRAPHIC IS 'Question1.3d2.png'
The Gaussian derivative filter decreased the error rate by $0.0048\,(17\%)$ which is significant. The Gaussian derivative filter smooths out noise in the image and creates a cleaner gradient edge, which will certainly help binning, creating better features.
\item[e)]
The hyperparameter $c$ (cost) (SVM) required some adjustment. To choose an optimal $c$ that gives a good trade-off between generalizability and accuracy, we held out 10,000 examples from the training set and used it as cross-validation set, and ran different $c$ values, and decided a $c$ value of $10$ offered good performance.
\end{itemize}
\newpage
\section*{1.4}
THE GRAPHIC IS 'misclassifcation60k.png'.
The errors on the classifier do seem pretty reasonable. About half of them seem to be ones that we can classify pretty quickly, but the other half are ones where we have to think a bit.
\newpage
\section*{1.6}
We ran orientation histograms with window size $4x4$ and obtained a 0.0158 error rate on the full training set.
\end{document}